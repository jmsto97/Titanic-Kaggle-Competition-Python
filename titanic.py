# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UQohxJnHx6EZsqsjOxFNq-MPdUqE_wR1
"""

# Attack plan for Sunday
# FIGURE OUT GOOGLE COLAB
# 1 Inspect the data and look for anything that needs cleaning
# Decide which models to do:
# General Exploratory Analysis
# What graphs do we want?
# Linear Regression, Decision Tree, Random Forest, KNN, GLMs
# What are our performance metrics?

from google.colab import drive
drive.mount('/content/gdrive')

# Titanic Kaggle /w Adrian and James

# https://github.com/Currie32/Titanic-Kaggle-Competition

import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LinearRegression


train = pd.read_csv('/content/gdrive/MyDrive/Kaggle/Titanic-Kaggle-Competition-master/train.csv')
test = pd.read_csv('/content/gdrive/MyDrive/Kaggle/Titanic-Kaggle-Competition-master/test.csv', engine='python')
train.head() #reads first few lines

train.describe() #summary of the data

# Figure out what we're missing
train.isnull().sum()

# Percentage of missing Age and Cabin data (separate).
display(train['Age'].isnull().sum()/len(train))
display(train['Cabin'].isnull().sum()/len(train))

missing_age = train['Age'].isnull()
missing_cabin = train['Cabin'].isnull()

#Plot missing values in Age and Cabin to get a better idea of what data might be useful.
missing_values = train.isnull().sum() / len(train)
missing_values = missing_values[missing_values > 0]
missing_values.sort_values(inplace=True)
missing_values
missing_values = missing_values.to_frame()
missing_values.columns = ['count']
missing_values.index.names = ['Name']
missing_values['Name'] = missing_values.index
sns.set(style="whitegrid", color_codes=True)
sns.barplot(x = 'Name', y = 'count', data=missing_values)
plt.show()

# Make a new dataset without cabin and binarise gender.
clean_train = train.drop(columns=['Cabin'])
clean_train['Sex'].replace('female', 0, inplace=True)
clean_train['Sex'].replace('male', 1, inplace=True)
display(clean_train.head())

numeric_train = clean_train.drop(columns=['Name', 'Ticket', 'Embarked'])
display(numeric_train.head())


categorical_train = clean_train.drop(columns=['Survived', 'Pclass', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare'])
display(categorical_train.head())

# Import 
imp = IterativeImputer(n_nearest_features=None, imputation_order='ascending')
imputed_train = pd.DataFrame(imp.fit_transform(numeric_train), columns = numeric_train.columns) #only apply imputer to numeric columns
display(imputed_train.head())
display(imputed_train['Age'].isnull().sum()/len(numeric_train))

# Join df to main dataset so that imputation will make sense. 
comp_train = pd.merge(categorical_train, imputed_train)
comp_train.head()

# Count of how many people came from each port.
comp_train = comp_train.dropna(subset=['Embarked'])

comp_train['Embarked'].value_counts()

fig = plt.figure()
#embarked_count = (comp_train[['Embarked']].value_counts(S), comp_train[['Embarked']].value_counts('Q'), comp_train[['Embarked']].value_counts('C') )
embarked_count = ((comp_train['Embarked'].values=='S').sum(), (comp_train['Embarked'].values=='C').sum(), (comp_train['Embarked'].values=='Q').sum())
x_lab = comp_train['Embarked'].unique()
ax = fig.add_axes([0,0,1,1])
ax.bar(x_lab, embarked_count,)

plt.show()
# Come back for review
# How to auto sort in descending rather than manually.

# Binarise Embarked values.
comp_train['Embarked'].replace('S', 0, inplace=True)
comp_train['Embarked'].replace('C', 1, inplace=True)
comp_train['Embarked'].replace('Q', 2, inplace=True)
comp_train.head()

# Ticket change
# Goin through each concerned value and see what info we can draw.
# Ticket number correlates to people sharing same family name.
# Want to factorise tickets but automate it
  #display(comp_train['Ticket'].value_counts())
  #comp_train.loc[comp_train['Ticket'] == 'CA 2144']
# Decided to use name because may not be able to identify families 
comp_train.head()

#comp_train['Surname'] = comp_train['Name'].str.split(',').str[0]
#comp_train.head()

#comp_train['Surname'] = comp_train['Surname'].astype('category')
#comp_train.head()

#a = pd.factorize(comp_train.Surname)
#comp_train['factor_surname'] = pd.factorize(comp_train.Surname)[0]
#comp_train.head()
#display(comp_train['factor_surname'].value_counts())

#Factorised ticket because there may be matching surname but no relation.
comp_train['factor_ticket'] = pd.factorize(comp_train.Ticket)[0]
comp_train.head()

# Outliers
#sns.boxplot(x=comp_train['Age'])
comp_train['Age'].min()

# Removing ages below zero as a result of logic.
comp_train = comp_train[comp_train['Age'] >= 0]
comp_train['Age'].min()

cleco = comp_train.drop(columns=['Name', 'Ticket'])
cleco.min()

#Cleaning done.

# Partitioning data
from sklearn.model_selection import train_test_split

# Y value will be survived, X will be everything besides survived.
# Split data into a train and test set 70/30
xvar = cleco.drop(columns=['Survived', 'PassengerId'])
yvar = cleco[['Survived']]
yvar.head()
x_train, x_test, y_train, y_test = train_test_split(
    xvar, yvar, test_size=0.3, random_state=1)

# 3/10/2020 Research classification predictor models, Classification or Regression.
# Classification; Logistic Regression (Similar to linear regression but for probability), Decision Tree, Random Forest, etc.

# First Model
# Logistic Regression
# We use Logistic Regression because it predicts probabilities for a binary dependent variable. We choose a probability threshold to decide whether or not a variable is likely to be a 1.
# In this case we pick a probability threshold that someone will survive.
# https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8
# Above link used for below code/s.
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import confusion_matrix

logreg = LogisticRegression()
# This logistic regression uses a default l2 regularisation, adds a penalty to certain variables that might heavily skew your results.  
# Lasso removes variables that are too similar. Ridge reduces or minimises a variable
# Lasso finds too impactful variable will make beta values 0 while Ridge will reduce it CLOSE to 0.
# Logistic and Linear are almost the same except logistic uses logit(y) 
logreg.fit(x_train, y_train)

y_pred = logreg.predict(x_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))

# Top Left = True Pos, Top Right = False Pos, Bottom Left = False Neg, Bottom Right = True Neg
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)
# Precision Ratio: tp/(tp+fp) = 88%
# Recall Ratio: tp/(tp+fn) = 81%

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# 0 Indicates non-survivors, 1 indicates survivors.

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test)) # Comparing logistic regression prediction to actual values.
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

# We want a larger area covered below the curve. Tells us good job :).

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import confusion_matrix
logreg2 = LogisticRegression(penalty='none')
# Lasso (betas to 0)
logreg2.fit(x_train, y_train)

y_pred2 = logreg2.predict(x_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg2.score(x_test, y_test)))

# Top Left = True Pos, Top Right = False Pos, Bottom Left = False Neg, Bottom Right = True Neg
confusion_matrix2 = confusion_matrix(y_test, y_pred2)
print(confusion_matrix2)
# Precision Ratio: tp/(tp+fp) = 
# Recall Ratio: tp/(tp+fn) =

# Decision Tree
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

tree = DecisionTreeClassifier()
tree = tree.fit(x_train, y_train)
y_pred3 = tree.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred3))

from sklearn import tree as dt
#Come back to this plotting shit.

dt.plot_tree(tree)

# Decision Tree ROC.
tree_roc_auc = roc_auc_score(y_test, tree.predict(x_test)) # Comparing logistic regression prediction to actual values.
fpr, tpr, thresholds = roc_curve(y_test, tree.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Decision Tree (area = %0.2f)' % tree_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Tree_ROC')
plt.show()

# Random Forest
from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
forest=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
forest.fit(x_train,y_train)
#RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                      #  criterion='gini', max_depth=None, max_features='auto',
                      #  max_leaf_nodes=None, max_samples=None,
                      #  min_impurity_decrease=0.0, min_impurity_split=None,
                      #  min_samples_leaf=1, min_samples_split=2,
                      #  min_weight_fraction_leaf=0.0, n_estimators=100,
                      #  n_jobs=None, oob_score=False, random_state=None,
                      #  verbose=0, warm_start=False)
y_pred4 = forest.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred4))

# Random Forest ROC.
forest_roc_auc = roc_auc_score(y_test, forest.predict(x_test)) # Comparing logistic regression prediction to actual values.
fpr, tpr, thresholds = roc_curve(y_test, forest.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % forest_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Forest_ROC')
plt.show()

# Feature selection similar to lasso but removes unused variables.

# Support Vector Machines (Linear)
from sklearn import svm
suplin = svm.SVC()
suplin.fit(x_train, y_train)
y_pred5 = suplin.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred5))
#Due to poor accuracy we decided not to further pursue this model.

try:
    test = pd.read_csv(os.path.join(pd.read_csv('/content/gdrive/MyDrive/Kaggle/Titanic-Kaggle-Competition-master/'), 'test.csv'),
                        sep='|', compression = 'gzip', dtype='unicode', error_bad_lines=False)
except CParserError:
    print("Something wrong the file")
return test

